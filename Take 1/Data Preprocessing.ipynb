{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "In this jupyter notebook I will explain the following data preprocessing techniques using a simple example dataset.\n",
    "\n",
    "* Handling missing data\n",
    "* Encoding categorical data\n",
    "* Splitting dataset\n",
    "* Feature scaling\n",
    "\n",
    "**Example Dataset:**\n",
    "Assume that you are a product owner and you would like to sell your product. Since you have sold this product before, you have the past customer details such as their ***age***, ***salary***, ***country*** and if they ***purchased*** the product or not. You are required to use this data to train a model in order to predict whether or not a new customer will purchase the product.\n",
    "\n",
    "Before training the model, we need to process the data in such a way that it is suitable and efficient for training.\n",
    "\n",
    "Please note that we only deal with the data preprocessing step in this notebook and not the model training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                             # for performing operating system dependent functionalities\n",
    "import numpy as np                    # for performing scientific computations\n",
    "import pandas as pd                   # for data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFilePath = os.getcwd() + '/Data.csv'\n",
    "dataset = pd.read_csv(dataFilePath)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Dataset\n",
    "\n",
    "We observe the following from the dataset\n",
    "* The independent and dependent values are in the same dataset.\n",
    "* The column of ***Age*** and ***Salary*** have missing values(represented as 'NaN').\n",
    "* The dataset contains both numbers and text i.e., both continuous and categorical data.\n",
    "* All the numbers in the dataset are not in the same scale.\n",
    "\n",
    "Let's start by separating the independent and dependent data.\n",
    "        In our example, we can see that the customer's behaviour(Purchased) is governed by his Country, Age and Salary.      Therefore, 'Purchased' is the dependent data and the rest of them are independent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,:-1].values   # Independent variable --> Country, Age, Salary\n",
    "y = dataset.iloc[:,-1].values    # Dependent variable   --> Purchased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data\n",
    "\n",
    "This can be done in one of the following two ways.\n",
    "\n",
    "***Remove the rows/columns with missing data:***\n",
    " This might not be a suitable approach if it contains any crucial information required in training model/ decision making.\n",
    "        \n",
    "***Replace the rows/columns with missing data:***\n",
    "This is the common approach. Here, the missing values are replaced with the mean/ median/ most_frequent values with respect to the column/ row. For this purpose we use scikit-learn, the machine learning library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# strategy can be either 'mean', 'meadian' or 'most_frequent'\n",
    "# axis = 0 indicates impute along columns (1 for rows)\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)  # Imputer object\n",
    "imputer = imputer.fit(X[:,1:3])                                         # compute(fit) the value to replace with\n",
    "X[:,1:3] = imputer.transform(X[:,1:3])                                  # replace(transform) with the fitted value \n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the missing values are replaced with the mean of the respective columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data\n",
    "\n",
    "In our dataset we have two categorical variables -- **Country** and **Purchased**. Country variable has three categorical data i.e., ***France, Spain*** and ***Germany***. Purchased variable has two categorical data i.e., ***Yes*** and ***No***. The machine learning models are based on mathematical equations therefore, we can intuitively understand that it is difficult to perform mathematical operations on the textual data. Hence, the need for encoding the categorical data.\n",
    "\n",
    "We start with encoding the categorical variable Country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [2, 27.0, 48000.0],\n",
       "       [1, 30.0, 54000.0],\n",
       "       [2, 38.0, 61000.0],\n",
       "       [1, 40.0, 63777.77777777778],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 38.77777777777778, 52000.0],\n",
       "       [0, 48.0, 79000.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder_X = LabelEncoder()                         # LabelEncoder object\n",
    "X[:,0] = labelencoder_X.fit_transform(X[:,0])           # fit and tranform the categorical variable - Country\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the categorical values France, Germany and Spain are encoded as 0, 1 and 2 respectively. Now, they are suitable for using in the mathematical equations. But, there is a problem. When these encoded values are used in the mathematical equations, the machine leaning model will think that Spain(2) > Germany(1) > France(0). But we know that this is not the case. Spain, Germany and France are just three categories with no relational order. To take care of this problem, we create dummy encodings. The number of columns in the dummy encoding is equal to the number of categories. This style of encoding is called ***OneHot encoding***.\n",
    "\n",
    "<img src=\"OneHotEncoding.png\" width=\"400px\" height=\"300px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.40000000e+01,\n",
       "        7.20000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 2.70000000e+01,\n",
       "        4.80000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 3.00000000e+01,\n",
       "        5.40000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.80000000e+01,\n",
       "        6.10000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.00000000e+01,\n",
       "        6.37777778e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.50000000e+01,\n",
       "        5.80000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.87777778e+01,\n",
       "        5.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.80000000e+01,\n",
       "        7.90000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 5.00000000e+01,\n",
       "        8.30000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.70000000e+01,\n",
       "        6.70000000e+04]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# categorical_features = [0] indicates the index of the column \n",
    "# that is to be onehot encoded\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])        # OneHotEncoder object\n",
    "X = onehotencoder.fit_transform(X).toarray()                     # fit and tranform with one hot encodings\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that country column is replaced by three new columns.\n",
    "\n",
    "Now, let's perform encoding for the Purchased variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_Y = LabelEncoder()                   # LabelEncoder object\n",
    "y = labelencoder_Y.fit_transform(y)               # fit and tranform the categorical variable - Purchased\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model knows that this(y) is a dependent variable. It considers the encoding 0(No) and 1(Yes) as categories. Hence, we do not need to use OneHot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset\n",
    "\n",
    "The machine learning model trains/ learns the co-relations from the dataset. If the model learns too much from the dataset, then it is called ***overfitting*** . If the model is overfits, then inspite of it's high training accuracy the model will fail to perform on the new dataset with slightly different co-relations. In order to know whether the model has actually learnt or memorized from the dataset, we split the dataset into training set, which is used for training the model and testing set, which is used for testing purpose. If the training accuracy and the testing accuracy are close enough then it indicates that the model has trained well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.00000000e+01,\n",
       "        6.37777778e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.70000000e+01,\n",
       "        6.70000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 2.70000000e+01,\n",
       "        4.80000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.87777778e+01,\n",
       "        5.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.80000000e+01,\n",
       "        7.90000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.80000000e+01,\n",
       "        6.10000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.40000000e+01,\n",
       "        7.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.50000000e+01,\n",
       "        5.80000000e+04]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size = 0.2, indicates that 20% of the dataset will be used as test data and 80% as training data\n",
    "# 'X_train, X_test, Y_train, Y_test', the order of specifying the train and test variables is very important\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "Often in the dataset, we can notice that the values will be of different magnitudes, units and ranges. Some machine learning models, while training use Euclidian distance between the datapoints. This means that the features with higher magnitude will have higher euclidean distance compared to the feature with smaller magnitude. This may lead to the domination of higher magnitude feature while training. To avoid this we need to scale/ normalize/ standardize the features to the same range.\n",
    "\n",
    "There are different methods used for feature scaling. The most commonly used method is ***Standardization***.\n",
    "\n",
    "$ x_{stand} = \\frac{x - mean(x)}{standard deviation(x)} $\n",
    "\n",
    "In our dataset we can see that the salary attribute has higher magnitude than that of age. Also, the data values are in different range. Hence, we need to standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n",
      "*************************************************************\n",
      "[[-1.          2.64575131 -0.77459667 -1.45882927 -0.90166297]\n",
      " [-1.          2.64575131 -0.77459667  1.98496442  2.13981082]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_X = StandardScaler()                   # StandardScaler object\n",
    "\n",
    "# fit --> compute the mean(m) and standard deviation(sd) from the training data.\n",
    "# transform --> use the computed m and sd values to scale/ standardize the training data\n",
    "X_train = sc_X.fit_transform(X_train)     \n",
    "\n",
    "# Note: the test data is only tranformed but NOT fitted\n",
    "# tranform --> use the same m and sd values computed from training data to scale/ standardize the testing data\n",
    "X_test = sc_X.transform(X_test)           \n",
    "\n",
    "print(X_train)\n",
    "print(\"*************************************************************\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/ Template\n",
    "\n",
    "All the above discussed methods are consolidated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "# Import libraries #\n",
    "#------------------#\n",
    "import os                             \n",
    "import numpy as np                    \n",
    "import pandas as pd   \n",
    "\n",
    "#-----------------#\n",
    "# Access the data #\n",
    "#-----------------#\n",
    "dataFilePath = os.getcwd() + '/Data.csv'\n",
    "dataset = pd.read_csv(dataFilePath)\n",
    "\n",
    "X = dataset.iloc[:,:-1].values   # Independent variable \n",
    "y = dataset.iloc[:,-1].values    # Dependent variable   \n",
    "\n",
    "#-----------------------#\n",
    "# Handling Missing Data #\n",
    "#-----------------------#\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)  # Imputer object\n",
    "imputer = imputer.fit(X[:,1:3])                                         # compute(fit) the value to replace with\n",
    "X[:,1:3] = imputer.transform(X[:,1:3])                                  # replace(transform) with the fitted value \n",
    "\n",
    "#---------------------------#\n",
    "# Encoding Categorical Data #\n",
    "#---------------------------#\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder_X = LabelEncoder()                                  # LabelEncoder object\n",
    "X[:,0] = labelencoder_X.fit_transform(X[:,0])                    # fit and tranform the categorical variable\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])        # OneHotEncoder object\n",
    "X = onehotencoder.fit_transform(X).toarray()                     # fit and tranform with one hot encodings\n",
    "\n",
    "labelencoder_Y = LabelEncoder()                                  # LabelEncoder object\n",
    "y = labelencoder_Y.fit_transform(y)                              # fit and tranform the categorical variable\n",
    "\n",
    "#-----------------------#\n",
    "# Splitting the Dataset #\n",
    "#-----------------------#\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#-----------------#\n",
    "# Feature Scaling #\n",
    "#-----------------#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_X = StandardScaler()                   # StandardScaler object\n",
    "X_train = sc_X.fit_transform(X_train)     # fit and transform the training data\n",
    "X_test = sc_X.transform(X_test)           # only tranform the testing data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
